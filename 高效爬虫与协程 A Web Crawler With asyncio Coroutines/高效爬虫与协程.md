# A Web Crawler With asyncio Coroutines

这是[开源程序架构](http://aosabook.org/en/index.html)系列的第四本[500 Lines or Less](https://github.com/aosabook/500lines/blob/master/README.md)的早期章节。
如果你发现任何问题，可以在我们的[Github追踪器](https://github.com/aosabook/500lines/issues)上反馈。
请关注[AOSA blog](http://aosabook.org/blog/)或新的章节和最后的出版计划，新闻公告[推特](https://twitter.com/aosabook), 获取关于本书的最新消息。
***

A. Jesse Jiryu Davis is a staff engineer at MongoDB in New York. He wrote Motor, the async MongoDB Python driver,
 and he is the lead developer of the MongoDB C Driver and a member of the PyMongo team. He contributes to asyncio and Tornado. 
 He writes at http://emptysqua.re.
A. Jesse Jiryu Davis在纽约为MongoDB工作。他编写了Motor，异步MongoDB Python驱动器，他也是MongoDB C驱动器的首席开发者，同时他也是PyMango组织的成员之一。
他对asyncio和Tornado同样有着杰出贡献。他的博客是 http://emptysqua.re .

Guido van Rossum is the creator of Python, one of the major programming languages on and off the web. 
The Python community refers to him as the BDFL (Benevolent Dictator For Life), a title straight from a Monty Python skit. 
Guido's home on the web is http://www.python.org/~guido/.
Guido van Rossum，Python之父，Python是目前主要的编程语言之一，无论线上线下。
Python社区是他生命里仁慈的独裁者，一个来自Monty Python短剧的标题。
Guido网上的家是http://www.python.org/~guido/ .

## 介绍

Classical computer science emphasizes efficient algorithms that complete computations as quickly as possible. 
But many networked programs spend their time not computing, but holding open many connections that are slow, 
or have infrequent events. These programs present a very different challenge: to wait for a huge number of network events efficiently.
A contemporary approach to this problem is asynchronous I/O, or "async".
经典计算机科学看重高效的算法以便能尽快完成计算。但是许多网络程序消耗时间不在计算上，而是维持打开许多缓慢的连接，或者有其他不频繁的事件。
这些程序表达了一个不同的挑战：如何高效的监听大量网络事件。一个现代的方法是异步I/O.

This chapter presents a simple web crawler. The crawler is an archetypal async application because it waits for many responses,
but does little computation. The more pages it can fetch at once, the sooner it completes. 
If it devotes a thread to each in-flight request, then as the number of concurrent requests rises it will run out of memory or 
other thread-related resource before it runs out of sockets. It avoids the need for threads by using asynchronous I/O.
这一章节实现了一个简单的网络爬虫。爬虫是一个异步应用因为它需要等待许多响应，而极少有计算。它每次可以抓取的页面越多，它完成的时间越久。
如果它为每一个运行的请求分发一个进程，那么随着并发请求数量的增加，它会耗尽内存，或者在它之前的线程相关的程序用光套接字。
它通过使用异步IO来避免大量进程需求。

We present the example in three stages. First, we show an async event loop and sketch a crawler that uses the event loop 
with callbacks: it is very efficient, but extending it to more complex problems would lead to unmanageable spaghetti code. 
Second, therefore, we show that Python coroutines are both efficient and extensible. 
We implement simple coroutines in Python using generator functions. In the third stage, 
we use the full-featured coroutines from Python's standard "asyncio" library1, and coordinate them using an async queue.
我们通过三步来实现这个例子。首先，我们展示一个异步事件循环并且梗概一个通过回掉使用这个事件循环。它非常高效，但是扩展它去适应更复杂的问题时会
导致难以处理的意大利面条式代码。因而接下来我们展示既高效又易扩展的Python协同程序。我们在Python中使用生成器函数来实现简单的协调程序。
最后，我们使用来自Python标准“asyncio”库中的全功能的协程程序，然后使用异步序列来整合他们。

## 任务