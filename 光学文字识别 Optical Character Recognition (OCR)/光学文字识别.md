# Optical Character Recognition (OCR)


这是[开源程序架构](http://aosabook.org/en/index.html)系列的第四本[《500 Lines or Less》](https://github.com/aosabook/500lines/blob/master/README.md)的早期章节。
如果你发现任何问题，可以在我们的[Github Issues](https://github.com/aosabook/500lines/issues)上反馈。
请关注[AOSA blog](http://aosabook.org/blog/)获取最新的章节及出版计划，还可以在[这个推特](https://twitter.com/aosabook)上获取关于本书的最新消息。

----
## 简介

要是你的电脑能帮你洗盘子，洗衣服，做饭，收拾房间那会是怎样一番景象呢？
我敢肯定，大多数人会很喜欢这样的一个得力助手！
但是，计算机如何才能准确的像人一样,按照人完成这些工作的方式，完成这些任务呢？

著名的计算机科学家艾伦·图灵提出了图灵测试。
这是一种可以判断一台机器的智能程度是否已经与人类匹敌的测试。
图灵测试需要一个处在明处提问者，两个处在暗处的未知实体（一个是人,一个是机器）。
提问者需要提出问题并试图通过未知实体的回答来分辨出哪一个是人，哪一个是机器。
如果提问者是无法辨别机器和人类，那么说明被测试的机器已经具有了与人类相仿水平的智能。

虽然关于‘图灵测试’的有效性以及人们能否创造出这种水平的智能一直有着很多争议，
但是毋庸置疑的是现在已经存在了一些具有不俗智能的机器。
如今一些软件已经可以操控机器人完成简单的办公任务，或者帮助那些爱滋海默症（老年痴呆）的患者。
还有一些人工智能更加常见，像谷歌的搜索引擎以及Facebook的热点推荐系统等等。 

一个比较普遍的人工智能的应用是光学字符识别（OCR）。
OCR系统可以识别手写的文字，并且将其转换成机器可以处理的字符。
当你将一个手写支票导入银行设备中验证存款金额时，你有考虑过这些神奇的功能到底是怎么实现的吗?
本章将带你了解一个基于智能神经网络的（ANN）的数字识别OCR系统是如何运作的。

在开始一切工作之前，我们还要明确一些背景知识。

## 什么是人工智能

尽管图灵先生关于智能的定义看起来很合理，但是关于智能的构成，归根结底是一种哲学层面的思索。
计算机科学家们已经（怎么分的不重要）将一些系统和算法分类到人工智能的某些分支当中。
关于这些分类，我们举几个例子（当然还有[更多](http://www-formal.stanford.edu/jmc/whatisai/node2.html)）：

* 根据真实世界已知信息进行的概率性演绎推理。例如：模糊推论法可以帮助恒温器在感知到空气温热潮湿时做出打开空调的决定；
* 启发式搜索。例如：在下棋时可以通过搜索所有可能的来选择出获益最高的走法；
* 反馈的机器学习模型。例如：一些模式识别问题，像OCR。

总之，机器学习就是使用大量数据进行模式识别系统的训练。
用于训练的数据集可能有两种：一种是示踪的，是说根据输入限定系统的期望输出,也就是对于某种输入有人为的期望结果；
一种是非示踪的，也就是说对系统的期望输出没有限制。
使用非示踪数据集的算法被称作非监督式算法，使用示踪数据集的算法则被称作监督式算法。
可以用来实现OCR的机器学习算法核技术有很多，今天我们所用ANNs就是其中一种很简单的方法。

## 人工神经网络（ANNs）

### <span id='what-are-anns'>什么是ANNs</span>
人工神经网络（ANNs）是一种由很多相互连通且可以互相通讯的节点组成的网络结构。
这种结构和功能是受到了生物脑结构的启发。
[赫布理论](http://www.nbb.cornell.edu/neurobio/linster/BioNB420/hebb.pdf)解释了生物体内的这些神经网络是如何通过改变物理层面的结构和连接强度来进行模式识别的学习的。
同样的，一个典型的ANN（如图13.1所示）的节点之间具有一些会根据网络的学习不断改变权值的连接（边）。
我们称节点标记+1为偏移。
最左边蓝色的一列节点是输入节点，中间一行包含很多隐藏节点，最右边的是输出节点。
中间包含很多隐藏节点的这一列，通常别叫作“隐藏层”。


![Figure 13.1](./img/Figure_13.1.jpg)


在图13.1中所有节点圆圈中的数字代表节点的输出。如果我们称第L层第n个节点的输出为n(L),第L层第i个节点与第L+1层第j个节点的链接称为![](./img/WijL.png)。那么，![](./img/a22.png)的输出为：

![function](./img/a22f.png)

其中，f()是激活函数，b是偏移量。激活函数决定着节点输出的类型。偏移量是一个用于提高神经网络精确性的加性节点。更多有关上述二者的细节可以参看[设计一个前馈神经网络](http://aosabook.org/en/500L/optical-character-recognition-ocr.html#sec.ocr.feedforward)

这类没有循环的神经网络我们称之为‘前馈’神经网络。节点的输出反馈回输入端的人工神经网络被则称为周期神经网络。有很多可以用于训练前馈神经网络的算法,*反向传播*就是其中比较常用的之一。在本章的OCR系统中就将使用反向传播的方式实现神经网络的。


### 怎么使用ANNs

和其他机器语言方法一样，使用反向传播的第一步就是将我们的问题化简。我们需要思考如何让我们的问题化简为一个可以用ANN解决的问题。具体一点,我们需要决定如何将我们的数据变换处理成能够输入到ANN之中的状态。对于这次要实现的OCR系统来说，我们可以将给定*数字的图片*中像素的位置作为输入。这种思路看起来非常直白简单，但是更多时候选择输入的格式并不会像这样简单。举个例子，如果我们想在一些比较大的图像中进行形状的识别，我们可能需要先对图像进行预处理来识别其中物体的大概轮廓，然后再将这个识别出的轮廓作为输入数据。

一旦我们决定了输入数据的格式，接下来要做些什么呢？就像我们在<a href=#what-are-anns>什么是人工神经网络</a>小节中所讲的，因为反向传播是一种监督算法，所以他需要使用被标记的数据进行训练。因此，当我们使用像素的位置作为训练输入时，同时也要将其所代表的的数字也如输入其中。这意味着我们需要收集大量的包含数字图像及其所代表的数字的数据对。

下一步是将数据分成训练组及校验组。训练组的数据用于驱动反向传播算法设置ANN的节点权值。校验组的数据有两个作用，一是用于校验经过训练组数据训练后的ANN的输出结果，二是评估算法性能（准确性）。如果我们打算比较反向传播算法与其他类似算法在处理我们的数据时的性能，我们可以分出50%的数据作为训练组，25%用于比较两种算法的性能（校验组），剩余25%用于测试最终选取的算法的准确性（测试组）。如果我们不打算实验多种算法，那我们可以把25%用于比较算法性能的数据分到训练组中，剩余的25%作为校验组检测ANN最终被训练的如何。

评估ANN的准确性有着双重的目的。首先，评估ANN的准确性可以避免过拟合。所谓过拟合就是指，训练出来网络对训练组数据的识别准确性远高于对校验组的识别准确性。当出现过拟合状态时，说明我们选取的训练数据不够有代表性，比较片面。这时，我们需要重新提炼训练用的数据。另外，通过评估不同隐藏层及隐藏节点的ANN，可以帮助我们确定最佳的ANN规模。合适的ANN规模是指，在保证识别率的情况下使用最少的节点/连接，以此来减小计算开支，加快训练及识别的效率。一旦确定了合适的ANN规模并完成训练，我们的ANN就准备好进行识别预测了。

## 在一个简单的OCR系统中设计决策方案
刚刚我们了解了一下基础的前馈神经网络,以及如何使用它们。接下来我们将一起探索一下如何搭建一个OCR系统。

首先,我们必须明确我们的系统能够实现什么功能。简单来讲,当我的用户手写了一个数字作为输入时,我们的系统可以利用这个图片进行训练或者根据图片识别出其中数字。
虽然我们的OCR系统可以在单机运行,那但是创建一个客户端-服务器的框架会比较灵活。使用服务器我们可以利用网络社群的数据训练我们的神经网,同时也可以将复杂密集的运算交给性能更加强劲的服务器。

我们的OCR系统主要由5部分组成,分别写在5个文件之中。它们分别是:

* 客户端(ocr.js)

* 服务器(server.py)

* 简单的用户界面(ocr.html)

* 基于反向传播训练的ANN(ocr.py)

* ANN的实现脚本(neural_network_design.py)

我们的用户界面非常简单:一个可以在其中绘制数字的画板,和一个可以切换训练还是识别的按钮。
客户端将会收集用户在用户界面绘制的数字图像,将其转换为一个数组,并将数组结合'训练/识别'指令传递给服务器。服务器简单的按照指令生成ANN模块的API请求。
ANN模块在第一次初始化时,会利用已有的数据进行训练,并将训练后的权值存在一个文件中。在之后再次调用ANN模块时,ANN模块会自动读取权值文件来恢复之前的训练状态。
这个模块包含训练及识别的核心逻辑运算。最后,设计一个通过实验不同的隐藏节点数量寻求最佳性能的决策脚本。将上述的这些模块组合在一起,我们就可以实现一个简单但却强大的OCR系统了。

这样一来我们,我们的基本设计思路就很清晰了。接下来,让我们动起手来将思想转化为代码。

### 简单的界面(orc.html)

我们在前文中已经讲到,首先我们需要收集一些用于训练ANN的数据。我们可以上传一系列手写的数字的图片到服务器中,但是这样做非常的麻烦。
比较好的替代方案就是,直接让用户在HTML的画布(Canvas)元素中绘制数字。我们可以给用户一个用于选择'训练/识别'的按钮。如果选择了训练,用户还需要将绘制的图片所代表的数字一并提交。
下面是我们的HTML代码:

    <html>
    <head>
        <script src="ocr.js"></script>
        <link rel="stylesheet" type="text/css" href="ocr.css">
    </head>
    <body onload="ocrDemo.onLoadFunction()">
        <div id="main-container" style="text-align: center;">
            <h1>OCR Demo</h1>
            <canvas id="canvas" width="200" height="200"></canvas>
            <form name="input">
                <p>Digit: <input id="digit" type="text"> </p>
                <input type="button" value="Train" onclick="ocrDemo.train()">
                <input type="button" value="Test" onclick="ocrDemo.test()">
                <input type="button" value="Reset" onclick="ocrDemo.resetCanvas();"/>
            </form> 
        </div>
    </body>
    </html>

### OCR客户端(ocr.js)

网页中的一个像素非常的难看清,所以我们可以用一个10x10真实像素的方块代表我们识别中的一个像素单位。这样一个200x200的画布(Canvas)对我们的ANN来说就相当于一个20x20的画布(Canvas)了。

    var ocrDemo = {
    CANVAS_WIDTH: 200,
    TRANSLATED_WIDTH: 20,
    PIXEL_WIDTH: 10, // TRANSLATED_WIDTH = CANVAS_WIDTH / PIXEL_WIDTH
    
为了看上去更清楚,我们可以为我们的像素单位填充一些颜色。我们使用`drawGrid()`方法来生成蓝色的网格。

    drawGrid: function(ctx) {
        for (var x = this.PIXEL_WIDTH, y = this.PIXEL_WIDTH; 
                 x < this.CANVAS_WIDTH; x += this.PIXEL_WIDTH, 
                 y += this.PIXEL_WIDTH) {
            ctx.strokeStyle = this.BLUE;
            ctx.beginPath();
            ctx.moveTo(x, 0);
            ctx.lineTo(x, this.CANVAS_WIDTH);
            ctx.stroke();

            ctx.beginPath();
            ctx.moveTo(0, y);
            ctx.lineTo(this.CANVAS_WIDTH, y);
            ctx.stroke();
        }
    },
    
我们需要将我们收集到的图像数据处理成能传递到服务器的格式。我们用0代表黑色像素,1代表白色像素,将数据储存在一个叫`data`的数组中。我们还需要监听鼠标在画布上的运动,来触发`fillSquare()`方法。`fillSquare()`方法可以将用户选中的像素上色。利用一些简单的运算,我们就能够将用户鼠标的绘制轨迹转化为我们画布中的像素信息了。

    onMouseMove: function(e, ctx, canvas) {
        if (!canvas.isDrawing) {
            return;
        }
        this.fillSquare(ctx, 
            e.clientX - canvas.offsetLeft, e.clientY - canvas.offsetTop);
    },

    onMouseDown: function(e, ctx, canvas) {
        canvas.isDrawing = true;
        this.fillSquare(ctx, 
            e.clientX - canvas.offsetLeft, e.clientY - canvas.offsetTop);
    },

    onMouseUp: function(e) {
        canvas.isDrawing = false;
    },

    fillSquare: function(ctx, x, y) {
        var xPixel = Math.floor(x / this.PIXEL_WIDTH);
        var yPixel = Math.floor(y / this.PIXEL_WIDTH);
        this.data[((xPixel - 1)  * this.TRANSLATED_WIDTH + yPixel) - 1] = 1;

        ctx.fillStyle = '#ffffff';
        ctx.fillRect(xPixel * this.PIXEL_WIDTH, yPixel * this.PIXEL_WIDTH, 
            this.PIXEL_WIDTH, this.PIXEL_WIDTH);
    },
    
现在,我们离目标越来越近了!接下来,我们需要一个对传输到服务器的训练数据进行处理的函数。就像下面这个`train()`函数。它会对数据进行错误排查,之后将数据写入`trainArray`中,最后通过调用`sendData()`函数将数据发送给服务器。
```javascript
    train: function() {
        var digitVal = document.getElementById("digit").value;
        if (!digitVal || this.data.indexOf(1) < 0) {
            alert("Please type and draw a digit value in order to train the network");
            return;
        }
        this.trainArray.push({"y0": this.data, "label": parseInt(digitVal)});
        this.trainingRequestCount++;

        // Time to send a training batch to the server.
        if (this.trainingRequestCount == this.BATCH_SIZE) {
            alert("Sending training data to server...");
            var json = {
                trainArray: this.trainArray,
                train: true
            };

            this.sendData(json);
            this.trainingRequestCount = 0;
            this.trainArray = [];
        }
    },
```

这里有一个值得一提的有趣设计,就是`trainingRequestCount`, `trainArray`, 和`BATCH_SIZE`的使用。
为什么这么讲呢,`BATCH_SIZE`是一个预定义的常量,用于限定客户端在一次性将成批的数据传递至服务期之前的最大缓存容量。
这样设计的主要原因是为了避免大量请求对服务器造成过大的压力。

如果存在许多客户端（例如，许多用户在`ocr.html`页面训练系统），或者如果客户端中存在另一个层，它接收扫描的绘制数字并将它们转换为像素以训练网络，则“BATCH_SIZE”为1将导致许多不必要的请求。 这种方法为客户端带来了灵活性，然而，在实践中存在需要时，服务端也需要执行批处理。 因为服务端可能会遭到DoS攻击，其中恶意客户端有意地向服务器发送许多请求以淹没它，就会使得它崩溃。

我们还需要一个test（）函数。与train（）类似，它应该对数据的有效性进行简单检查并将其发送出去。 然而，对于test（），不需要批处理，因为用户请求预测时应该会得到一个即时的结果。

```javascript
    test: function() {
        if (this.data.indexOf(1) < 0) {
            alert("Please draw a digit in order to test the network");
            return;
        }
        var json = {
            image: this.data,
            predict: true
        };
        this.sendData(json);
    },
```

最后，我们将需要一些函数来进行HTTP POST请求，接收响应，并在处理任何可能的错误。

```javascript
    receiveResponse: function(xmlHttp) {
        if (xmlHttp.status != 200) {
            alert("Server returned status " + xmlHttp.status);
            return;
        }
        var responseJSON = JSON.parse(xmlHttp.responseText);
        if (xmlHttp.responseText && responseJSON.type == "test") {
            alert("The neural network predicts you wrote a \'" 
                   + responseJSON.result + '\'');
        }
    },

    onError: function(e) {
        alert("Error occurred while connecting to server: " + e.target.statusText);
    },

    sendData: function(json) {
        var xmlHttp = new XMLHttpRequest();
        xmlHttp.open('POST', this.HOST + ":" + this.PORT, false);
        xmlHttp.onload = function() { this.receiveResponse(xmlHttp); }.bind(this);
        xmlHttp.onerror = function() { this.onError(xmlHttp) }.bind(this);
        var msg = JSON.stringify(json);
        xmlHttp.setRequestHeader('Content-length', msg.length);
        xmlHttp.setRequestHeader("Connection", "close");
        xmlHttp.send(msg);
    }
```

### 服务端 (`server.py`)

尽管这只是一个中继信息的小型服务器，但是我们仍然需要考虑如何接收和处理HTTP请求。 首先，我们需要决定使用什么样的HTTP请求。 在最后一节中，客户端使用的是POST请求，但是为什么我们决定使用POST呢？由于数据被发送到服务器，PUT或POST请求是最合适的。我们只需要发送一个不带任何URL参数的json包。所以在理论上，GET请求也能满足我们的需求，但在语义上这样却是没有意义的。在程序员之间长期以来一直有着关于选择PUT和POST的争论;对于这个问题KNPLabs有一个非常有[趣的总结](https://knpuniversity.com/screencast/rest/put-versus-post)。

另一个需要考虑的问题是将“训练”和“预测”请求发送到不同端点（例如`http://localhost/train`和`http://localhost/predict`），还是发送到同一端点进行处理。在这种情况下，我们可以使用后一种方法，因为训练和预测对数据执行的操作之间的差别很小，所以一个简单的if语句就可以非常完美的适应不同的情况。在实践中，如果服务器对每个请求类型进行的处理有很大的差别，那么最好将它们作为单独的端点。这个决定，同时影响着如何设计网页返回的错误信息。例如，当在有效载荷中既没有指定“train”或“predict”时，返回400“错误请求”错误。但是如果使用单独的端点，就不会有这样的问题。如果OCR系统在后台执行的处理由于任何原因而失败，或者服务器没有正确处理发送的信息，则返回500“内部服务器错误”。同样，如果端点是分开的，则将有更多的空间来详细地发送更适当的错误。例如，确定内部服务器错误实际上是由错误请求引起的等等。

最后，我们需要决定什么时候在哪里初始化OCR系统。在server.py中服务器启动之前进行初始化是一个不错的方法。因为在第一次运行时，OCR系统需要在对某些预插入的的数据进行网络训练，这可能需要几分钟。如果服务器在此处理完成之前启动，则任何训练或预测的请求都会抛出异常，因为在这种情况的情况下，OCR对象尚未被初始化。当然还有另一种实现方式，预先产生一些不准确的初始ANN用于前几个查询，同时在后台不断的异步的训练ANN。这种替代方法允许立即使用ANN，但是实现更复杂，并且如果服务器被重置，它将仅在服务器启动时及时保存。这种类型的实现对于需要高可用性的OCR服务将更有益。

这里我们的服务器主要的代码是在一个小巧的函数中处理POST请求。

```python
    def do_POST(s):
        response_code = 200
        response = ""
        var_len = int(s.headers.get('Content-Length'))
        content = s.rfile.read(var_len);
        payload = json.loads(content);

        if payload.get('train'):
            nn.train(payload['trainArray'])
            nn.save()
        elif payload.get('predict'):
            try:
                response = {
                    "type":"test", 
                    "result":nn.predict(str(payload['image']))
                }
            except:
                response_code = 500
        else:
            response_code = 400

        s.send_response(response_code)
        s.send_header("Content-type", "application/json")
        s.send_header("Access-Control-Allow-Origin", "*")
        s.end_headers()
        if response:
            s.wfile.write(json.dumps(response))
        return
```

### 设计前馈式神经网络 (`neural_network_design.py`)
\label{sec.ocr.feedforward}
When designing a feedforward ANN, there are a few factors we must consider. The
first is what activation function to use. We mentioned activation functions
earlier as the decision-maker for a node’s output. The type of the decision an
activation function makes will help us decide which one to use. In our case, we
will be designing an ANN that outputs a value between 0 and 1 for each digit
(0-9). Values closer to 1 would mean the ANN predicts this is the drawn digit
and values closer to 0 would mean it’s predicted to not be the drawn digit.
Thus, we want an activation function that would have outputs either close to 0
or close to 1. We also need a function that is differentiable because we will
need the derivative for our backpropagation computation. A commonly used
function in this case is the sigmoid because it satisfies both these
constraints. StatSoft provides a [nice
list](http://www.fmi.uni-sofia.bg/fmi/statist/education/textbook/eng/glosa.html)
of common activation functions and their properties.

A second factor to consider is whether we want to include biases. We've
mentioned biases a couple of times before but haven't really talked about what
they are or why we use them. Let's try to understand this by going back to how
the output of a node is computed in \aosafigref{500l.ocr.ann}. Suppose we had a single input
node and a single output node, our output formula would be $y = f(wx)$, where $y$
is the output, $f()$ is the activation function, $w$ is the weight for the link
between the nodes, and $x$ is the variable input for the node. The bias is
essentially a node whose output is always $1$. This would change the output
formula to $y = f(wx + b)$ where $b$ is the weight of the connection between the
bias node and the next node. If we consider $w$ and $b$ as constants and $x$ as a
variable, then adding a bias adds a constant to our linear function input to
$f(.)$.

Adding the bias therefore allows for a shift in the $y$-intercept and in general
gives more flexibility for the output of a node. It's often good practice to
include biases, especially for ANNs with a small number of inputs and outputs.
Biases allow for more flexibility in the output of the ANN and thus provide the
ANN with more room for accuracy. Without biases, we’re less likely to make
correct predictions with our ANN or would need more hidden nodes to make more
accurate predictions.

Other factors to consider are the number of hidden layers and the number of
hidden nodes per layer. For larger ANNs with many inputs and outputs, these
numbers are decided by trying different values and testing the network's
performance. In this case, the performance is measured by training an ANN of a
given size and seeing what percentage of the validation set is classified
correctly. In most cases, a single hidden layer is sufficient for decent
performance, so we only experiment with the number of hidden nodes here.

```python
# Try various number of hidden nodes and see what performs best
for i in xrange(5, 50, 5):
    nn = OCRNeuralNetwork(i, data_matrix, data_labels, train_indices, False)
    performance = str(test(data_matrix, data_labels, test_indices, nn))
    print "{i} Hidden Nodes: {val}".format(i=i, val=performance)
```

Here we initialize an ANN with between 5 to 50 hidden nodes in increments of 5.
We then call the `test()` function.

```python
def test(data_matrix, data_labels, test_indices, nn):
    avg_sum = 0
    for j in xrange(100):
        correct_guess_count = 0
        for i in test_indices:
            test = data_matrix[i]
            prediction = nn.predict(test)
            if data_labels[i] == prediction:
                correct_guess_count += 1

        avg_sum += (correct_guess_count / float(len(test_indices)))
    return avg_sum / 100
```

The inner loop is counting the number of correct classifications which are then
divided by the number of attempted classifications at the end. This gives a
ratio or percentage accuracy for the ANN. Since each time an ANN is trained,
its weights may be slightly different, we repeat this process 100 times in the
outer loop so we can take an average of this particular ANN configuration's
accuracy. In our case, a sample run of `neural_network_design.py` looks like the
following:

```
PERFORMANCE
-----------
5 Hidden Nodes: 0.7792
10 Hidden Nodes: 0.8704
15 Hidden Nodes: 0.8808
20 Hidden Nodes: 0.8864
25 Hidden Nodes: 0.8808
30 Hidden Nodes: 0.888
35 Hidden Nodes: 0.8904
40 Hidden Nodes: 0.8896
45 Hidden Nodes: 0.8928
```

From this output we can conclude that 15 hidden nodes would be most optimal.
Adding 5 nodes from 10 to 15 gets us ~1% more accuracy, whereas improving the
accuracy by another 1% would require adding another 20 nodes. Increasing the
hidden node count also increases computational overhead. So it would take
networks with more hidden nodes longer to be trained and to make predictions.
Thus we choose to use the last hidden node count that resulted in a dramatic
increase in accuracy. Of course, it’s possible when designing an ANN that
computational overhead is no problem and it's top priority to have the most
accurate ANN possible. In that case it would be better to choose 45 hidden
nodes instead of 15.

### Core OCR Functionality

In this section we’ll talk about how the actual training occurs via
backpropagation, how we can use the network to make predictions, and other key
design decisions for core functionality.

#### Training via Backpropagation (`ocr.py`)

We use the backpropagation algorithm to train our ANN. It consists of 4 main
steps that are repeated for every sample in the training set, updating the ANN
weights each time.

First, we initialize the weights to small (between -1 and 1) random values. In
our case, we initialize them to values between -0.06 and 0.06 and store them in
matrices `theta1`, `theta2`, `input_layer_bias`, and `hidden_layer_bias`. Since
every node in a layer links to every node in the next layer we can create a
matrix that has m rows and n columns where n is the number of nodes in one
layer and m is the number of nodes in the adjacent layer. This matrix would
represent all the weights for the links between these two layers. Here theta1
has 400 columns for our 20x20 pixel inputs and `num_hidden_nodes` rows.
Likewise, `theta2` represents the links between the hidden layer and output
layer. It has `num_hidden_nodes` columns and `NUM_DIGITS` (`10`) rows. The
other two vectors (1 row), `input_layer_bias` and `hidden_layer_bias` represent
the biases.

```python
    def _rand_initialize_weights(self, size_in, size_out):
        return [((x * 0.12) - 0.06) for x in np.random.rand(size_out, size_in)]
```

```python
            self.theta1 = self._rand_initialize_weights(400, num_hidden_nodes)
            self.theta2 = self._rand_initialize_weights(num_hidden_nodes, 10)
            self.input_layer_bias = self._rand_initialize_weights(1, 
                                                                  num_hidden_nodes)
            self.hidden_layer_bias = self._rand_initialize_weights(1, 10)

```

The second step is _forward propagation_, which is essentially computing the
node outputs as described in \aosasecref{sec.ocr.ann}, layer by layer starting from
the input nodes. Here, `y0` is an array of size 400 with the inputs we wish to
use to train the ANN. We multiply `theta1` by `y0` transposed so that we have two
matrices with sizes `(num_hidden_nodes x 400) * (400 x 1)` and have a resulting
vector of outputs for the hidden layer of size num_hidden_nodes. We then add
the bias vector and apply the vectorized sigmoid activation function to this
output vector, giving us `y1`. `y1` is the output vector of our hidden layer. The
same process is repeated again to compute `y2` for the output nodes. `y2` is now
our output layer vector with values representing the likelihood that their
index is the drawn number. For example if someone draws an 8, the value of `y2`
at the 8th index will be the largest if the ANN has made the correct
prediction. However, 6 may have a higher likelihood than 1 of being the drawn
digit since it looks more similar to 8 and is more likely to use up the same
pixels to be drawn as the 8. `y2` becomes more accurate with each additional
drawn digit the ANN is trained with.

```python
    # The sigmoid activation function. Operates on scalars.
    def _sigmoid_scalar(self, z):
        return 1 / (1 + math.e ** -z)
```

```python
            y1 = np.dot(np.mat(self.theta1), np.mat(data['y0']).T)
            sum1 =  y1 + np.mat(self.input_layer_bias) # Add the bias
            y1 = self.sigmoid(sum1)

            y2 = np.dot(np.array(self.theta2), y1)
            y2 = np.add(y2, self.hidden_layer_bias) # Add the bias
            y2 = self.sigmoid(y2)
```

The third step is _back propagation_, which involves computing the errors at the
output nodes then at every intermediate layer back towards the input. Here we
start by creating an expected output vector, `actual_vals`, with a `1` at the index
of the digit that represents the value of the drawn digit and `0`s otherwise. The
vector of errors at the output nodes, `output_errors`, is computed by subtracting
the actual output vector, `y2`, from `actual_vals`. For every hidden layer
afterwards, we compute two components. First, we have the next layer’s
transposed weight matrix multiplied by its output errors. Then we have the
derivative of the activation function applied to the previous layer. We then
perform an element-wise multiplication on these two components, giving a vector
of errors for a hidden layer. Here we call this `hidden_errors`.

```python
            actual_vals = [0] * 10 
            actual_vals[data['label']] = 1
            output_errors = np.mat(actual_vals).T - np.mat(y2)
            hidden_errors = np.multiply(np.dot(np.mat(self.theta2).T, output_errors), 
                                        self.sigmoid_prime(sum1))
```

Weight updates that adjust the ANN weights based on the errors computed
earlier. Weights are updated at each layer via matrix multiplication. The error
matrix at each layer is multiplied by the output matrix of the previous layer.
This product is then multiplied by a scalar called the learning rate and added
to the weight matrix. The learning rate is a value between 0 and 1 that
influences the speed and accuracy of learning in the ANN. Larger learning rate
values will generate an ANN that learns quickly but is less accurate, while
smaller values will will generate an ANN that learns slower but is more
accurate. In our case, we have a relatively small value for learning rate, 0.1.
This works well since we do not need the ANN to be immediately trained in order
for a user to continue making train or predict requests. Biases are updated by
simply multiplying the learning rate by the layer’s error vector.

```python
            self.theta1 += self.LEARNING_RATE * np.dot(np.mat(hidden_errors), 
                                                       np.mat(data['y0']))
            self.theta2 += self.LEARNING_RATE * np.dot(np.mat(output_errors), 
                                                       np.mat(y1).T)
            self.hidden_layer_bias += self.LEARNING_RATE * output_errors
            self.input_layer_bias += self.LEARNING_RATE * hidden_errors
```

#### Testing a Trained Network (`ocr.py`)

Once an ANN has been trained via backpropagation, it is fairly straightforward
to use it for making predictions. As we can see here, we start by computing the
output of the ANN, `y2`, exactly the way we did in step 2 of backpropagation.
Then we look for the index in the vector with the maximum value. This index is
the digit predicted by the ANN.

```
    def predict(self, test):
        y1 = np.dot(np.mat(self.theta1), np.mat(test).T)
        y1 =  y1 + np.mat(self.input_layer_bias) # Add the bias
        y1 = self.sigmoid(y1)

        y2 = np.dot(np.array(self.theta2), y1)
        y2 = np.add(y2, self.hidden_layer_bias) # Add the bias
        y2 = self.sigmoid(y2)

        results = y2.T.tolist()[0]
        return results.index(max(results))
```

#### Other Design Decisions (`ocr.py`)
Many resources are available online that go into greater detail on the
implementation of backpropagation. One good resource is from a [course by the
University of
Willamette](http://www.willamette.edu/~gorr/classes/cs449/backprop.html). It
goes over the steps of backpropagation and then explains how it can be
translated into matrix form. While the amount of computation using matrices is
the same as using loops, the benefit is that the code is simpler and easier to
read with fewer nested loops. As we can see, the entire training process is
written in under 25 lines of code using matrix algebra.

As mentioned in the introduction of \aosasecref{sec.ocr.decisions}, persisting
the weights of the ANN means we do not lose the progress made in training it
when the server is shut down or abruptly goes down for any reason. We persist
the weights by writing them as JSON to a file. On startup, the OCR loads the
ANN’s saved weights to memory. The save function is not called internally by
the OCR but is up to the server to decide when to perform a save. In our case,
the server saves the weights after each update. This is a quick and simple
solution but it is not optimal since writing to disk is time consuming. This
also prevents us from handling multiple concurrent requests since there is no
mechanism to prevent simultaneous writes to the same file. In a more
sophisticated server, saves could perhaps be done on shutdown or once every few
minutes with some form of locking or a timestamp protocol to ensure no data
loss.

```python
    def save(self):
        if not self._use_file:
            return

        json_neural_network = {
            "theta1":[np_mat.tolist()[0] for np_mat in self.theta1],
            "theta2":[np_mat.tolist()[0] for np_mat in self.theta2],
            "b1":self.input_layer_bias[0].tolist()[0],
            "b2":self.hidden_layer_bias[0].tolist()[0]
        };
        with open(OCRNeuralNetwork.NN_FILE_PATH,'w') as nnFile:
            json.dump(json_neural_network, nnFile)

    def _load(self):
        if not self._use_file:
            return

        with open(OCRNeuralNetwork.NN_FILE_PATH) as nnFile:
            nn = json.load(nnFile)
        self.theta1 = [np.array(li) for li in nn['theta1']]
        self.theta2 = [np.array(li) for li in nn['theta2']]
        self.input_layer_bias = [np.array(nn['b1'][0])]
        self.hidden_layer_bias = [np.array(nn['b2'][0])]
```

## Conclusion
Now that we’ve learned about AI, ANNs, backpropagation, and building an
end-to-end OCR system, let’s recap the highlights of this chapter and the big
picture.

We started off the chapter by giving background on AI, ANNs, and roughly what
we will be implementing. We discussed what AI is and examples of how it’s used.
We saw that AI is essentially a set of algorithms or problem-solving approaches
that can provide an answer to a question in a similar manner as a human would.
We then took a look at the structure of a Feedforward ANN. We learned that
computing the output at a given node was as simple as summing the products of
the outputs of the previous nodes and their connecting weights. We talked about
how to use an ANN by first formatting the input and partitioning the data into
training and validation sets.

Once we had some background, we started talking about creating a web-based,
client-server system that would handle user requests to train or test the OCR.
We then discussed how the client would interpret the drawn pixels into an array
and perform an HTTP request to the OCR server to perform the training or
testing. We discussed how our simple server read requests and how to design an
ANN by testing performance of several hidden node counts. We finished off by
going through the core training and testing code for backpropagation.

Although we’ve built a seemingly functional OCR system, this chapter simply
scratches the surface of how a real OCR system might work. More sophisticated
OCR systems could have pre-processed inputs, use hybrid ML algorithms, have
more extensive design phases, or other further optimizations.
